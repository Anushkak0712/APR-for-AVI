{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1mbci9PuWaqE72QYSfKdGB96ZrgO6vixO","authorship_tag":"ABX9TyM0l0WWU6ZDGggv+Dmlsqov"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\n","import os\n","import pickle\n","\n","def merge_large_dat_files(input_dir, output_file):\n","    try:\n","        files = sorted([f for f in os.listdir(input_dir) if f.endswith('.dat')])  # Sort for consistency\n","        print(f\"Found .dat files: {files}\")\n","\n","        if not files:\n","            raise ValueError(\"No .dat files found in the directory.\")\n","\n","        merged_data = []  # List to store all training samples\n","\n","        for filename in files:\n","            file_path = os.path.join(input_dir, filename)\n","            try:\n","                with open(file_path, 'rb') as f:\n","                    while True:\n","                        try:\n","                            data = pickle.load(f)  # Load object from file\n","                            if isinstance(data, list) and all(isinstance(item, tuple) for item in data):\n","                                merged_data.extend(data)  # Append to list\n","                                print(f\"✔ Merged: {filename} ({len(data)} samples)\")\n","                            else:\n","                                print(f\"❌ Skipped {filename} (unexpected format)\")\n","                        except EOFError:\n","                            break  # Stop reading when end of file is reached\n","            except (pickle.UnpicklingError, EOFError) as e:\n","                print(f\"❌ Corrupted file: {filename}, skipping. Error: {e}\")\n","\n","        # Save merged data\n","        with open(output_file, 'wb') as f_out:\n","            pickle.dump(merged_data, f_out)\n","\n","        print(f\"✅ Merged data saved to: {output_file} (Total: {len(merged_data)} samples)\")\n","\n","    except Exception as e:\n","        print(f\"Unexpected error: {e}\")\n","        import traceback\n","        traceback.print_exc()  # Print full error details\n","\n"],"metadata":{"id":"RkmQzB-IfAX9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","input_directory = '/content/drive/MyDrive/new_dats_val/test_dats'\n","output_file = \"/content/drive/MyDrive/merged_dats/merged_test_data.dat\"\n","\n","merge_large_dat_files(input_directory, output_file)"],"metadata":{"id":"q2ztclcWe0A-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_directory = '/content/drive/MyDrive/new_dats_val/val_dats'\n","output_file = \"/content/drive/MyDrive/merged_dats/merged_val_data.dat\"\n","merge_large_dat_files(input_directory, output_file)"],"metadata":{"id":"GGJ_3sYHgmxU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743098057325,"user_tz":-330,"elapsed":62433,"user":{"displayName":"BE Project","userId":"08652001596415499870"}},"outputId":"7c113d16-f578-4783-cbeb-6bdc77cac1e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found .dat files: ['val_set_1val.dat', 'val_set_2val.dat']\n","✔ Merged: val_set_1val.dat (400 samples)\n","✔ Merged: val_set_2val.dat (400 samples)\n","✅ Merged data saved to: /content/drive/MyDrive/merged_dats/merged_val_data.dat (Total: 800 samples)\n"]}]},{"cell_type":"code","source":["input_directory = '/content/drive/MyDrive/new_dats'\n","output_file = \"/content/drive/MyDrive/merged_dats/merged_train_data4.dat\"\n","merge_large_dat_files(input_directory, output_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QRSRf_j7vcJ5","outputId":"84a8d5a2-01c5-44ff-fa18-4b157612f8ed","executionInfo":{"status":"ok","timestamp":1743187091566,"user_tz":-330,"elapsed":194314,"user":{"displayName":"BE Project","userId":"08652001596415499870"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found .dat files: ['Copy of training_set_6train.dat', 'training_set_2train.dat', 'training_set_3train.dat', 'training_set_4train.dat', 'training_set_5train.dat']\n","✔ Merged: Copy of training_set_6train.dat (400 samples)\n","✔ Merged: training_set_2train.dat (400 samples)\n","✔ Merged: training_set_3train.dat (400 samples)\n","✔ Merged: training_set_4train.dat (400 samples)\n","✔ Merged: training_set_5train.dat (400 samples)\n","✅ Merged data saved to: /content/drive/MyDrive/merged_dats/merged_train_data4.dat (Total: 2000 samples)\n"]}]}]}