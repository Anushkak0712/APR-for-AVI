{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"15sRaWsR-B0qKfqLauRrqbB-IXdlpEep0","authorship_tag":"ABX9TyPqz9RljHHn/moRMtS9aORf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install ffmpeg-python openai-whisper\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwX1DEV5D3or","executionInfo":{"status":"ok","timestamp":1745000613543,"user_tz":-330,"elapsed":115774,"user":{"displayName":"BE Project","userId":"08652001596415499870"}},"outputId":"752f5c4d-f4fd-4db3-ec3b-234d9f484e65","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ffmpeg-python\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n","Collecting openai-whisper\n","  Downloading openai-whisper-20240930.tar.gz (800 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (1.0.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n","Collecting tiktoken (from openai-whisper)\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n","Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803405 sha256=b2520fa65cb2e45ba619a290f5b442e9091f14318f95a23fe355c84275f4645f\n","  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ffmpeg-python, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed ffmpeg-python-0.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"]}]},{"cell_type":"code","source":["import os\n","import random\n","import numpy as np\n","import ffmpeg as ff\n","import cv2\n","import librosa\n","import torch\n","import whisper\n","from typing import Tuple, List, Dict\n","from transformers import BertTokenizer, BertModel\n","import tensorflow as tf\n","\n","# Load models once\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')\n","whisper_model = whisper.load_model(\"base.en\")\n","predictor = tf.keras.models.load_model('/content/drive/MyDrive/first_impressions_model.keras', safe_mode=False)\n","\n","\n","def extract_audio_from_video(file_path: str) -> np.ndarray:\n","    inputfile = ff.input(file_path)\n","    out = inputfile.output('-', format='f32le', acodec='pcm_f32le', ac=1, ar='44100')\n","    raw = out.run(capture_stdout=True, capture_stderr=True)\n","    return np.frombuffer(raw[0], np.float32)\n","\n","def preprocess_audio_series(audio_raw):\n","    print(f\"Original audio shape: {audio_raw.shape}\")\n","\n","    # Ensure the number of elements is divisible by 24 * 1319 (31656)\n","    target_size = 24 * 1319  # This is the required number of elements\n","\n","    # If the size of the audio is greater than required, truncate\n","    if audio_raw.size > target_size:\n","        audio_raw = audio_raw[:target_size]\n","    # If the size is smaller, pad with zeros\n","    elif audio_raw.size < target_size:\n","        padding = target_size - audio_raw.size\n","        audio_raw = np.pad(audio_raw, (0, padding), 'constant')\n","\n","    # Now reshape to (24, 1319, 1)\n","    audio_raw = np.reshape(audio_raw, (24, 1319, 1))\n","\n","    print(f\"Processed audio shape: {audio_raw.shape}\")\n","    return audio_raw\n","\n","\n","\n","def get_number_of_frames(file_path: str) -> int:\n","    try:\n","        # Check if file exists and is readable\n","        if not os.path.exists(file_path):\n","            raise ValueError(\"Video file does not exist\")\n","\n","        # Get file size\n","        file_size = os.path.getsize(file_path)\n","        if file_size == 0:\n","            raise ValueError(\"Video file is empty\")\n","\n","        # Use OpenCV to get actual frame count and properties\n","        cap = cv2.VideoCapture(file_path)\n","        if not cap.isOpened():\n","            raise ValueError(\"Could not open video file\")\n","\n","        # Get video properties\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        duration = total_frames / fps if fps > 0 else 0\n","\n","        # If we couldn't get frame count directly, try to count frames\n","        if total_frames <= 0:\n","            print(\"Could not get frame count directly, counting frames...\")\n","            total_frames = 0\n","            while True:\n","                ret, _ = cap.read()\n","                if not ret:\n","                    break\n","                total_frames += 1\n","            duration = total_frames / fps if fps > 0 else 0\n","            # Reset video capture\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n","\n","        print(f\"Video properties - Duration: {duration:.2f}s, FPS: {fps:.2f}, Total frames: {total_frames}\")\n","\n","        if total_frames <= 0:\n","            raise ValueError(\"Could not determine number of frames in video\")\n","\n","        cap.release()\n","        return total_frames\n","\n","    except Exception as e:\n","        print(f\"Error details: {str(e)}\")\n","        raise ValueError(f\"Error processing video file: {str(e)}\")\n","\n","def extract_N_video_frames(file_path: str, number_of_samples: int = 6) -> List[np.ndarray]:\n","    try:\n","        cap = cv2.VideoCapture(file_path)\n","        if not cap.isOpened():\n","            raise ValueError(\"Could not open video file\")\n","\n","        # Get video properties\n","        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","        duration = total_frames / fps if fps > 0 else 0\n","\n","        # If we couldn't get frame count directly, count frames\n","        if total_frames <= 0:\n","            print(\"Could not get frame count directly, counting frames...\")\n","            total_frames = 0\n","            frame_positions = []\n","            while True:\n","                ret, _ = cap.read()\n","                if not ret:\n","                    break\n","                frame_positions.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n","                total_frames += 1\n","            duration = total_frames / fps if fps > 0 else 0\n","            print(f\"Counted {total_frames} frames at positions: {frame_positions}\")\n","            # Reset video capture\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n","\n","        print(f\"Video properties - Duration: {duration:.2f}s, FPS: {fps:.2f}, Total frames: {total_frames}\")\n","\n","        if total_frames < number_of_samples:\n","            raise ValueError(f\"Video is too short. Expected at least {number_of_samples} frames, got {total_frames}\")\n","\n","        video_frames = []\n","        sample_size = min(number_of_samples, total_frames)\n","\n","        # Instead of using frame indices, sample at specific time points\n","        time_points = [duration * i / (sample_size - 1) for i in range(sample_size)]\n","        print(f\"Sampling at time points: {time_points}\")\n","\n","        for time_point in time_points:\n","            # Convert time to frame position\n","            frame_pos = int(time_point * fps)\n","            print(f\"Attempting to read frame at time {time_point:.2f}s (position {frame_pos})\")\n","\n","            # Try to read the frame\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)\n","            res, frame = cap.read()\n","\n","            if not res:\n","                print(f\"Warning: Could not read frame at position {frame_pos}\")\n","                # Try reading frames sequentially until we get a valid one\n","                for offset in range(-5, 6):  # Try 5 frames before and after\n","                    if frame_pos + offset < 0:\n","                        continue\n","                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos + offset)\n","                    res, frame = cap.read()\n","                    if res:\n","                        print(f\"Successfully read frame at offset {offset}\")\n","                        break\n","\n","                if not res:\n","                    raise ValueError(f\"Could not read any frames near position {frame_pos}\")\n","\n","            video_frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","\n","        cap.release()\n","        return video_frames\n","    except Exception as e:\n","        raise ValueError(f\"Error extracting video frames: {str(e)}\")\n","\n","def resize_image(image: np.ndarray, new_size: Tuple[int, int]) -> np.ndarray:\n","    return cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n","\n","def crop_image_window(image: np.ndarray, training: bool = False) -> np.ndarray:\n","    height, width, _ = image.shape\n","    N_index = (height - 128) // 2\n","    M_index = (width - 128) // 2\n","    return image[N_index:N_index+128, M_index:M_index+128, :]\n","\n","def get_text_embeddings(text: str) -> np.ndarray:\n","    if not text:\n","        return np.zeros((768,))\n","    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n","    with torch.no_grad():\n","        outputs = bert_model(**tokens)\n","    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n","\n","def predict_personality(file_path: str) -> Dict[str, float]:\n","    try:\n","        # Validate file exists and is readable\n","        if not os.path.exists(file_path):\n","            raise ValueError(\"Video file does not exist\")\n","\n","        # Get file size\n","        file_size = os.path.getsize(file_path)\n","        if file_size == 0:\n","            raise ValueError(\"Video file is empty\")\n","\n","        transcription = whisper_model.transcribe(file_path)['text']\n","        print(f\"Transcription: {transcription}\")\n","        print(f\"Predicting personality for file: {file_path}\")\n","\n","        # Audio\n","        audio_raw = extract_audio_from_video(file_path)\n","        audio_input = preprocess_audio_series(audio_raw)\n","\n","        # Video\n","        sampled = extract_N_video_frames(file_path, number_of_samples=6)\n","        resized_images = [resize_image(im, (248, 140)) for im in sampled]\n","        cropped_images = [crop_image_window(img) / 255.0 for img in resized_images]\n","        video_input = np.stack(cropped_images)\n","\n","        # Text\n","        text_embedding = get_text_embeddings(transcription)\n","\n","        # Predict\n","        preds = predictor.predict([\n","            np.expand_dims(audio_input, axis=0),\n","            np.expand_dims(video_input, axis=0),\n","            np.expand_dims(text_embedding, axis=0)\n","        ])[0]\n","\n","        traits = ['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n","        return dict(zip(traits, preds.tolist()))\n","    except Exception as e:\n","        raise ValueError(f\"Error processing video: {str(e)}\")\n"],"metadata":{"id":"rlqEL8QMIdrv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_personality(video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPJCS3KLJk4C","executionInfo":{"status":"ok","timestamp":1745001089986,"user_tz":-330,"elapsed":14825,"user":{"displayName":"BE Project","userId":"08652001596415499870"}},"outputId":"a2e6866e-e5d0-48dc-f431-ac81a4469fb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"stream","name":"stdout","text":["Transcription:  that sir you must be having a lot of option for this position to select but I don't know about the rest what I believe that I meet with the minimum requirement for this position but I'm able to do my job for its best requirement and apart from this my words cannot justify my action until I don't get the right opportunity to perform so I will be very privileged if you avail me this opportunity sir.\n","Predicting personality for file: /content/whyHire1.mp4\n","Original audio shape: (1219584,)\n","Processed audio shape: (24, 1319, 1)\n","Video properties - Duration: 27.63s, FPS: 30.01, Total frames: 829\n","Sampling at time points: [0.0, 5.52552, 11.05104, 16.57656, 22.10208, 27.6276]\n","Attempting to read frame at time 0.00s (position 0)\n","Attempting to read frame at time 5.53s (position 165)\n","Attempting to read frame at time 11.05s (position 331)\n","Attempting to read frame at time 16.58s (position 497)\n","Attempting to read frame at time 22.10s (position 663)\n","Attempting to read frame at time 27.63s (position 829)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'extraversion': 1.010462760925293,\n"," 'neuroticism': -13.511393547058105,\n"," 'agreeableness': -11.11119270324707,\n"," 'conscientiousness': 16.780467987060547,\n"," 'openness': -0.36275890469551086}"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["predict_personality('/content/WhatsApp Video 2025-04-18 at 10.08.43 PM.mp4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOk1SVhNMDt3","executionInfo":{"status":"ok","timestamp":1745001422847,"user_tz":-330,"elapsed":9481,"user":{"displayName":"BE Project","userId":"08652001596415499870"}},"outputId":"172e743b-4c1f-472b-aa84-81df5d82d46b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"stream","name":"stdout","text":["Transcription:  sort of not. I've just been really really busy school wise and friend wise and other stuff. But I am back. I'm working on some stuff, video ideas.\n","Predicting personality for file: /content/WhatsApp Video 2025-04-18 at 10.08.43 PM.mp4\n","Original audio shape: (674816,)\n","Processed audio shape: (24, 1319, 1)\n","Video properties - Duration: 15.30s, FPS: 30.00, Total frames: 459\n","Sampling at time points: [0.0, 3.06, 6.12, 9.180000000000001, 12.24, 15.3]\n","Attempting to read frame at time 0.00s (position 0)\n","Attempting to read frame at time 3.06s (position 91)\n","Attempting to read frame at time 6.12s (position 183)\n","Attempting to read frame at time 9.18s (position 275)\n","Attempting to read frame at time 12.24s (position 367)\n","Attempting to read frame at time 15.30s (position 459)\n","Warning: Could not read frame at position 459\n","Successfully read frame at offset -5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'extraversion': -0.340932697057724,\n"," 'neuroticism': -1.0145142078399658,\n"," 'agreeableness': 1.6222896575927734,\n"," 'conscientiousness': -0.23875777423381805,\n"," 'openness': 1.057740330696106}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["predict_personality('/content/WhatsApp Video 2025-04-18 at 9.42.17 PM.mp4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_G1KE6fNBUT","executionInfo":{"status":"ok","timestamp":1745001755109,"user_tz":-330,"elapsed":10222,"user":{"displayName":"BE Project","userId":"08652001596415499870"}},"outputId":"e715ef85-24be-46c6-df0e-5c89ffaf5498"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"stream","name":"stdout","text":["Transcription:  Contains monologue video type, 10,000 clips of 5 to 10 seconds across 2000 participants. Multimodal data combines audio, visual and text data for robust analysis. Big-fi annotations with data sets include labels for the ocean personality traits based on participants' responses. Big-fi annotations include ethnicity, gender and age annotations providing rich data for personality traits analysis.\n","Predicting personality for file: /content/WhatsApp Video 2025-04-18 at 9.42.17 PM.mp4\n","Original audio shape: (1680269,)\n","Processed audio shape: (24, 1319, 1)\n","Video properties - Duration: 38.27s, FPS: 29.47, Total frames: 1128\n","Sampling at time points: [0.0, 7.654600977198697, 15.309201954397395, 22.963802931596092, 30.61840390879479, 38.27300488599349]\n","Attempting to read frame at time 0.00s (position 0)\n","Attempting to read frame at time 7.65s (position 225)\n","Attempting to read frame at time 15.31s (position 451)\n","Attempting to read frame at time 22.96s (position 676)\n","Attempting to read frame at time 30.62s (position 902)\n","Attempting to read frame at time 38.27s (position 1128)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'extraversion': -0.5545593500137329,\n"," 'neuroticism': -1.5340781211853027,\n"," 'agreeableness': 1.7984704971313477,\n"," 'conscientiousness': -0.1540166586637497,\n"," 'openness': 1.2759473323822021}"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["predict_personality('/content/WhatsApp Video 2025-04-19 at 12.11.12 AM.mp4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJI90lePOYt8","executionInfo":{"status":"ok","timestamp":1745002036257,"user_tz":-330,"elapsed":15076,"user":{"displayName":"BE Project","userId":"08652001596415499870"}},"outputId":"673b4c54-4b9c-4989-f341-711934817941"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"stream","name":"stdout","text":["Transcription:  Hyamie for four reasons. The first reason is because I am a great fit for the job description, meaning you won't have to spend your valuable time training and supervising me. The second reason you should hire me is because I am a positive person who will bring fresh ideas to your team to help you grow. The third reason you should hire me is because I have a track record of success, which I will replicate for you in at this position. For example, in my last role, I helped the company increase sales by delivering excellent customer service. And the fourth reason you should hire me is because I am positive about change and will support you.\n","Predicting personality for file: /content/WhatsApp Video 2025-04-19 at 12.11.12 AM.mp4\n","Original audio shape: (1846272,)\n","Processed audio shape: (24, 1319, 1)\n","Video properties - Duration: 41.80s, FPS: 25.02, Total frames: 1046\n","Sampling at time points: [0.0, 8.36, 16.72, 25.08, 33.44, 41.8]\n","Attempting to read frame at time 0.00s (position 0)\n","Attempting to read frame at time 8.36s (position 209)\n","Attempting to read frame at time 16.72s (position 418)\n","Attempting to read frame at time 25.08s (position 627)\n","Attempting to read frame at time 33.44s (position 836)\n","Attempting to read frame at time 41.80s (position 1046)\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"]},{"output_type":"execute_result","data":{"text/plain":["{'extraversion': 0.2852325439453125,\n"," 'neuroticism': 0.4350805878639221,\n"," 'agreeableness': 0.4812329113483429,\n"," 'conscientiousness': 0.5682945251464844,\n"," 'openness': 0.41324323415756226}"]},"metadata":{},"execution_count":14}]}]}